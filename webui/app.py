import os
from dotenv import load_dotenv
import streamlit as st
from rag_client import ask_with_evidence

# .env ã‚’èª­ã¿è¾¼ã‚€ï¼ˆAPI ã‚­ãƒ¼ã€OpenAI/Jina/ãƒ­ãƒ¼ã‚«ãƒ« LLM ã®è¨­å®šãªã©ï¼‰
load_dotenv()

st.set_page_config(page_title="RAG Chat", page_icon="ğŸ—‚ï¸", layout="wide")

# ---- Sidebar: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ----
st.sidebar.header("è¨­å®š")
storage = st.sidebar.text_input("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜å…ˆ (storage)", value="storage")
k = st.sidebar.slider("Top-Kï¼ˆå–å¾—ã™ã‚‹æ–‡æ›¸æ•°ï¼‰", min_value=1, max_value=12, value=4, step=1)
llm_backend = st.sidebar.selectbox(
    "LLM ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", options=["openai", "internlm2", "none"], index=0,
    help="openai=ã‚¯ãƒ©ã‚¦ãƒ‰ / internlm2=OpenAI äº’æ›ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆï¼ˆLM Studioãƒ»Ollama ç­‰ï¼‰/ none=æŠ½å‡ºã®ã¿ï¼ˆç”Ÿæˆãªã—ï¼‰"
)
llm_model = st.sidebar.text_input(
    "LLM ãƒ¢ãƒ‡ãƒ«å", value="gpt-5-mini",
    help="ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒ none ã®å ´åˆã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚internlm2 ã¯ãƒ­ãƒ¼ã‚«ãƒ«/ç§æœ‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå´ã®ãƒ¢ãƒ‡ãƒ«åã«åˆã‚ã›ã¦ãã ã•ã„ã€‚"
)
rerank = st.sidebar.checkbox("Cross-Encoder å†ãƒ©ãƒ³ã‚¯ä»˜ã‘ã‚’æœ‰åŠ¹åŒ–", value=True,
                             help="ã‚ªãƒ•ã«ã™ã‚‹ã¨ --no-rerank ã‚’ä»˜ä¸ã—ã¾ã™ã€‚")

st.sidebar.markdown("---")
if st.sidebar.button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
    st.session_state.messages = []

# ---- Main: ãƒãƒ£ãƒƒãƒˆ UI ----
st.title("RAG Chat For Japan News(Streamlit)")
st.caption("ChatGPT é¢¨ã«ã‚³ãƒ¼ãƒ‘ã‚¹ã¸è³ªå•ã€‚OpenAI / äº’æ› API / æŠ½å‡ºã®ã¿ï¼ˆç”Ÿæˆãªã—ï¼‰ã«å¯¾å¿œã€‚")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ã“ã‚Œã¾ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æç”»
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
        if m.get("evidence"):
            with st.expander("ğŸ“ æ ¹æ‹ ã‚’è¦‹ã‚‹ï¼ˆãƒ’ãƒƒãƒˆã—ãŸæ–­ç‰‡ï¼‰", expanded=False):
                st.code(m["evidence"])

# å…¥åŠ›æ¬„
user_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ Enter â€¦")
if user_input:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # RAG ã‚’å‘¼ã³å‡ºã—
    with st.chat_message("assistant"):
        with st.spinner("æ¤œç´¢ã¨ç”Ÿæˆä¸­â€¦"):
            answer, evidence = ask_with_evidence(
                user_input, storage=storage, k=k,
                llm_backend=llm_backend, llm_model=llm_model, rerank=rerank
            )
            st.markdown(answer if answer else "_ï¼ˆçµæœãªã—ï¼å¤±æ•—ï¼‰_")
            if evidence:
                with st.expander("ğŸ“ æ ¹æ‹ ã‚’è¦‹ã‚‹ï¼ˆãƒ’ãƒƒãƒˆã—ãŸæ–­ç‰‡ï¼‰", expanded=False):
                    st.code(evidence)

    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå‡ºåŠ›ã‚’å±¥æ­´ã«ä¿å­˜
    st.session_state.messages.append({
        "role": "assistant",
        "content": answer if answer else "_ï¼ˆçµæœãªã—ï¼å¤±æ•—ï¼‰_",
        "evidence": evidence
    })
